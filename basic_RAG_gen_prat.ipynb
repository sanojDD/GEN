{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "SRBI5Tt1CN9H",
        "outputId": "6203d742-1973-469e-9359-90e2dab9d19e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install torch transformers faiss-cpu sentence-transformers --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import pickle\n",
        "\n",
        "documents = [\n",
        "    \"what is RAG?\",\n",
        "    \"RAG combines Retrieval and generation\",\n",
        "    \"open source LLM are powerful\"\n",
        "]"
      ],
      "metadata": {
        "id": "ia8QFAB8cXZN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "embeddings = model.encode(documents)"
      ],
      "metadata": {
        "id": "BQc2XJPKcvze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "index.add(embeddings)"
      ],
      "metadata": {
        "id": "_ImF-YEIdE-T"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('faiss_index.pkl', 'wb') as f:\n",
        "  pickle.dump((index, documents), f)"
      ],
      "metadata": {
        "id": "MesmV8oedSCF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
      ],
      "metadata": {
        "id": "Eo4trfENdfqq"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load llm\n",
        "!pip install transformers --upgrade --quiet"
      ],
      "metadata": {
        "id": "sottlg2Cdj_i"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "model_name = 'gpt2'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "generator = AutoModelForCausalLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "BO-I8oYHdvu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rag_pipeline(query):\n",
        "  retriever = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "  query_embedding = retriever.encode([query])\n",
        "  D, I = index.search(query_embedding, k=2)\n",
        "  retrieved_docs = [documents[i] for i in I[0]]\n",
        "\n",
        "  # generating responses\n",
        "  input_text = query + \" \" + \" \".join(retrieved_docs)\n",
        "  inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
        "  outputs = generator.generate(**inputs, max_length=100)\n",
        "  return tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "ItE8cDQneF_Y"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testing the pipeline\n",
        "\n",
        "query = \"what is RAG?\"\n",
        "print(rag_pipeline(query))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "jO_ewdDyfYCi",
        "outputId": "0f3cd11c-1b39-4ef6-ed1b-7c2893b1ea9e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "what is RAG? what is RAG? RAG combines Retrieval and generation of information. It is a powerful tool for the development of information. It is a tool for the development of information. It is a tool for the development of information. It is a tool for the development of information. It is a tool for the development of information. It is a tool for the development of information. It is a tool for the development of information. It is a tool for the development of\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XMc_lZ3Xfd3-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}